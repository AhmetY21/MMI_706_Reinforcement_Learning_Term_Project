{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import visualise_pricing_strategy, visualise_episode_rewards,visualise_demand_data, external_demand_function\n",
    "from Pricing_Environment import demand_calculator, action_strategy,pricing_env\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "product_config = {\n",
    "    \"min_price\": 10,\n",
    "    \"max_price\": 100,\n",
    "    \"initial_demand\": 0.5,\n",
    "}\n",
    "\n",
    "demand_calculator_config = {\n",
    "    \"price_probability_ranges\": {\n",
    "        (0, 50): 0.8,   # 80% demand probability for prices between $0 and $50\n",
    "        (51, 100): 0.6, # 60% demand probability for prices between $51 and $100\n",
    "        # Add more ranges a\n",
    "        # nd probabilities as needed\n",
    "    },\n",
    "    'low':0,\n",
    "    'high':100,\n",
    "    'seasonality':True\n",
    "}\n",
    "\n",
    "action_strategy_config = {\n",
    "    \"action_probabilities\": {\n",
    "        0: 0.1,  # Decrease price significantly\n",
    "        1: 0.2,  # Decrease price slightly\n",
    "        2: 0.4,  # Keep price\n",
    "        3: 0.2,  # Increase price slightly\n",
    "        4: 0.1,  # Increase price significantly\n",
    "    },\n",
    "    \"price_change_map\": {\n",
    "        0: -10,  # Decrease significantly\n",
    "        1: -5,   # Decrease slightly\n",
    "        2: 0,    # Keep price\n",
    "        3: 5,    # Increase slightly\n",
    "        4: 10    # Increase significantly\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU support) is available and choose accordingly\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mean = nn.Linear(256, action_dim)\n",
    "        self.log_std = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x)\n",
    "        std = torch.exp(log_std)  # Standard deviation must be positive\n",
    "        return mean, std\n",
    "\n",
    "    def sample(self, state):\n",
    "        mean, std = self(state)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        z = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
    "        action = torch.tanh(z) * self.max_action\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        return self.network(sa)\n",
    "\n",
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action, device):\n",
    "        self.device = device\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.critic1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic2_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=3e-4)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=3e-4)\n",
    "        self.discount = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.policy_delay = 2\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        action = self.actor.sample(state)\n",
    "        return action.cpu().data.numpy().flatten()\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        for it in range(batch_size):\n",
    "            # Sample a batch of transitions from the replay buffer\n",
    "            state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action = torch.FloatTensor(action).to(self.device)\n",
    "            next_state = torch.FloatTensor(next_state).to(self.device)\n",
    "            reward = torch.FloatTensor(reward).to(self.device)\n",
    "            done = torch.FloatTensor(done).to(self.device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            with torch.no_grad():\n",
    "                next_action = self.actor.sample(next_state)\n",
    "                target_Q1 = self.critic1_target(next_state, next_action)\n",
    "                target_Q2 = self.critic2_target(next_state, next_action)\n",
    "                target_Q = torch.min(target_Q1, target_Q2)\n",
    "                target_Q = reward + ((1 - done) * self.discount * target_Q)\n",
    "\n",
    "            # Get current Q estimates\n",
    "            current_Q1 = self.critic1(state, action)\n",
    "            current_Q2 = self.critic2(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "            # Optimize the critic\n",
    "            self.critic1_optimizer.zero_grad()\n",
    "            self.critic2_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic1_optimizer.step()\n",
    "            self.critic2_optimizer.step()\n",
    "\n",
    "            # Delayed policy updates\n",
    "            if it % self.policy_delay == 0:\n",
    "                # Compute actor loss\n",
    "                actor_loss = -self.critic1(state, self.actor.sample(state)).mean()\n",
    "\n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # Soft update the target networks\n",
    "                for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the environment is already imported and initialized\n",
    "env = pricing_env.PricingEnvironment(render_mode=\"text\", is_continuous=True, product_config=product_config, demand_calculator_config=demand_calculator_config, action_strategy_config=action_strategy_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the SAC Agent\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0] if env.is_continuous else env.action_space.n\n",
    "max_action = env.action_space.high[0] if env.is_continuous else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "SACAgent(state_dim=3, action_dim=1, max_action=1,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_term_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
